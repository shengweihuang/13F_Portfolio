{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Main code to scrape the 13F from SEC website**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Access SEC Website:**\n",
    "   - Using the CIK from the csv list, we access the SEC website with the URL: `https://www.sec.gov/cgi-bin/browse-edgar?CIK={cik}&owner=exclude&action=getcompany&type=13F-HR`.\n",
    "\n",
    "2. **Navigate to 13F Form:**\n",
    "   - We check filings from newest to oldest. If the latest filing date on the website is newer than or equal to the one in our csv list, we run the `scrap_and_save_report()` function to extract the data.\n",
    "\n",
    "3. **Extract Initial Data:**\n",
    "   - In the first layer, we use `findAll` to get `<a>` tags with `id='documentsbutton'` (saved in `tags`) and `<tr>` elements (saved in `rows`). The `tags` provide URLs for the next layer, while `rows` help determine if thereâ€™s a new or updated form.\n",
    "\n",
    "\n",
    "4. **Collect Key Information:**\n",
    "   - In the second layer, we extract the period of report, filing date, and accession number. We also get the `xml_url` needed to access the actual 13F form.\n",
    "\n",
    "\n",
    "5. **Retrieve 13F Data:**\n",
    "   - In the final layer, we use the `xml_url` to fetch the 13F form and extract all relevant data with `findAll`. The `scrap_and_save_report` function compiles this into a DataFrame and moves on to the next manager.\n",
    "\n",
    "6. **Process and Save Results:**\n",
    "   - The `process_ciks` function runs through all CIKs, calling `scrap_and_save_report` for each. It aggregates data for the same stock, returns a DataFrame of all managers' 13F forms, lists new filings, and save a new csv file with the latest information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Import Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import warnings\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Define Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the headers in order to get access to the SEC website, you can replace with your preferred email\n",
    "headers = {\n",
    "    'User-Agent': 'shengwei.huang@me.com',\n",
    "    'Accept-Encoding': 'gzip, deflate, br',\n",
    "    'HOST': 'www.sec.gov',\n",
    "}\n",
    "sec_url = 'https://www.sec.gov'\n",
    "\n",
    "\n",
    "def get_request(url):\n",
    "    return requests.get(url, headers=headers)\n",
    "\n",
    "# The main url we get access to all fund manager's 13F\n",
    "def create_url(cik):\n",
    "    return f'https://www.sec.gov/cgi-bin/browse-edgar?CIK={cik}&owner=exclude&action=getcompany&type=13F-HR'\n",
    "\n",
    "# Coverting the date format to we want\n",
    "def convert_date_format(date_str):\n",
    "    try:\n",
    "        return datetime.strptime(date_str, '%Y-%m-%d').strftime('%Y-%m-%d').upper()\n",
    "    except ValueError:\n",
    "        return date_str\n",
    "\n",
    "# Get the latest filing date to see whether run into the next action\n",
    "def get_latest_filing_date(cik):\n",
    "    try:\n",
    "        path = create_url(cik)\n",
    "        response = get_request(path)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        tags = soup.findAll('a', id='documentsbutton')\n",
    "        rows = soup.findAll('tr')\n",
    "\n",
    "        report_url = None\n",
    "        for i, row in enumerate(rows[5:]):\n",
    "            cols = row.findAll('td')\n",
    "            if len(cols) > 3 and '13F-HR' in cols[0].text: # WE get the latest row, to get it's filing date\n",
    "                report_url = sec_url + cols[1].a['href']\n",
    "                break\n",
    "\n",
    "        if report_url is None:\n",
    "            print(f\"No 13F-HR form found for CIK {cik}\")\n",
    "            return None\n",
    "\n",
    "        last_report = sec_url + tags[i]['href']\n",
    "        response_two = get_request(last_report)\n",
    "        soup_two = BeautifulSoup(response_two.text, \"html.parser\")\n",
    "\n",
    "        text_content = soup_two.get_text()\n",
    "        filing_date_match = re.search(r'Filing Date\\s*(\\d{4}-\\d{2}-\\d{2})', text_content)\n",
    "        filing_date = convert_date_format(filing_date_match.group(1)) if filing_date_match else \"N/A\"\n",
    "\n",
    "        return filing_date\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred for CIK {cik}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Main function to scrap all new 13F form for one fund manager\n",
    "def scrap_and_save_report(requested_cik, company_name, access):\n",
    "    try:\n",
    "        path = create_url(requested_cik)\n",
    "        warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4.builder', lineno=545)\n",
    "        response = get_request(path)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        tags = soup.findAll('a', id=\"documentsbutton\") # List for all second layer url\n",
    "        rows = soup.findAll('tr') # information to see whether go access to the next layer\n",
    "\n",
    "        report_url = None\n",
    "        filing_list = []\n",
    "        for i, row in enumerate(rows[5:]):\n",
    "            cols = row.findAll('td')\n",
    "            if len(cols) > 3 and '13F-HR' in cols[0].text:\n",
    "                if access in cols[2].text: # If the access match with the csv access number, which means that we already scrap this form, then we break the loop\n",
    "                    break                   \n",
    "                else:\n",
    "                    if '13F-HR/A' in cols[0].text:\n",
    "                        filing_list.append(\"13F-HR/A\")\n",
    "                    else:\n",
    "                        filing_list.append(\"13F-HR\")\n",
    "                    pass\n",
    "        if i == 0:\n",
    "            return None\n",
    "        \n",
    "        columns = [\n",
    "            \"FILING\",\n",
    "            \"ACCESSION_NUMBER\",\n",
    "            \"PERIODOFREPORT\",\n",
    "            \"FILING_DATE\",\n",
    "            \"FILINGMANAGER_NAME\",\n",
    "            \"CIK\",\n",
    "            \"CUSIP\",\n",
    "            \"TITLEOFCLASS\",\n",
    "            \"NAMEOFISSUER\",\n",
    "            \"SSHPRNAMT\",\n",
    "            \"VALUE\",\n",
    "        ]\n",
    "\n",
    "        df = pd.DataFrame(columns=columns)\n",
    "\n",
    "        for j in range(i): # i is the number of url we save in tags\n",
    "\n",
    "            # Second Layer\n",
    "            last_report = sec_url + tags[j]['href']\n",
    "            response_two = get_request(last_report)\n",
    "            soup_two = BeautifulSoup(response_two.text, \"html.parser\")\n",
    "            tags_two = soup_two.findAll('a', attrs={'href': re.compile('xml')}) # tags_two is the xml list in second layer\n",
    "            xml_url = tags_two[-1].get('href') # xml_url is the xml we want to get access to the third layer\n",
    "\n",
    "            text_content = soup_two.get_text() # in the second layer, we can get the Accession number, filing date, period of report.\n",
    "            acc_no_match = re.search(r'SEC Accession No. (\\d{10}-\\d{2}-\\d{6})', text_content)\n",
    "            acc_no = acc_no_match.group(1)\n",
    "            filing_date_match = re.search(r'Filing Date\\s*(\\d{4}-\\d{2}-\\d{2})', text_content)\n",
    "            filing_date = convert_date_format(filing_date_match.group(1)) if filing_date_match else \"N/A\"\n",
    "            period_of_report_match = re.search(r'Period of Report\\s*(\\d{4}-\\d{2}-\\d{2})', text_content)\n",
    "            period_of_report = convert_date_format(period_of_report_match.group(1)) if period_of_report_match else \"N/A\"\n",
    "\n",
    "            # Third Layer\n",
    "            response_xml = get_request(sec_url + xml_url) \n",
    "            soup_xml = BeautifulSoup(response_xml.content, \"lxml\")\n",
    "\n",
    "            # Get all the information we want in 13F form\n",
    "            issuers = soup_xml.body.findAll(re.compile('nameofissuer'))\n",
    "            titleofclasses = soup_xml.body.findAll(re.compile('titleofclass'))\n",
    "            cusips = soup_xml.body.findAll(re.compile('cusip'))\n",
    "            values = soup_xml.body.findAll(re.compile('value'))\n",
    "            sshprnamts = soup_xml.body.findAll(re.compile('sshprnamt'))\n",
    "\n",
    "            for k in range(len(issuers)):\n",
    "                cusip = str((cusips[k].text).upper().zfill(9))\n",
    "                df.loc[len(df)] = [\n",
    "                    filing_list[j],\n",
    "                    acc_no,\n",
    "                    period_of_report,\n",
    "                    filing_date,\n",
    "                    company_name,\n",
    "                    int(requested_cik),\n",
    "                    cusip,\n",
    "                    titleofclasses[k].text,\n",
    "                    issuers[k].text,\n",
    "                    int(sshprnamts[2 * k].text),\n",
    "                    int(values[k].text)\n",
    "                ]\n",
    "\n",
    "        return df # Return a DataFrame of all new update 13F form for one manager\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred for CIK {requested_cik}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Main Function\n",
    "def process_ciks(cik_company_df):\n",
    "    all_dfs = []\n",
    "    filing = []\n",
    "    no_filing = []\n",
    "\n",
    "    # According to the csv list, run all fund managers\n",
    "    for index, row in cik_company_df.iterrows():\n",
    "        requested_cik = str(row['CIK'])\n",
    "        access = str(row['ACCESSION_NUMBER'])\n",
    "        company_name = row['FILINGMANAGER_NAME']\n",
    "        \n",
    "        # Get the LATEST_FILING_DATE from our csv file\n",
    "        latest_csv_filing_date = row['LATEST_FILING_DATE']\n",
    "        latest_csv_filing_date = datetime.strptime(latest_csv_filing_date, '%d-%b-%y')\n",
    "\n",
    "        # Get the LATEST_FILING_DATE from website\n",
    "        latest_website_filing_date = get_latest_filing_date(requested_cik)\n",
    "        latest_website_filing_date = datetime.strptime(latest_website_filing_date, '%Y-%m-%d' ) if latest_website_filing_date else None\n",
    "\n",
    "        # To see if there is a new update from the manager, otherwise we just skip to the next manager\n",
    "        if latest_website_filing_date and latest_website_filing_date >= latest_csv_filing_date:\n",
    "            result_df = scrap_and_save_report(requested_cik, company_name, access)\n",
    "            if result_df is not None:\n",
    "\n",
    "                # combine the result_df to all_dfs, which contains all managers forms\n",
    "                all_dfs.append(result_df)\n",
    "\n",
    "                # Form a new csv file for LATEST_FILING_DATE and ACCESSION_NUMBER\n",
    "                cik_company_df.loc[index, 'LATEST_FILING_DATE'] = latest_website_filing_date.strftime('%d-%b-%y')\n",
    "                cik_company_df.loc[index, 'ACCESSION_NUMBER'] = result_df['ACCESSION_NUMBER'].iloc[0]\n",
    "                print(f\"New fillings for CIK {requested_cik}\")\n",
    "                filing.append(requested_cik)\n",
    "            else:\n",
    "                no_filing.append(requested_cik)\n",
    "                print(f\"No new filings for CIK {requested_cik}\")\n",
    "        else:\n",
    "            print(f\"No new filings for CIK {requested_cik}\")\n",
    "            no_filing.append(requested_cik)\n",
    "\n",
    "    return all_dfs,filing, no_filing\n",
    "\n",
    "def save_data_to_csv(all_dfs, output_dir, cik_company_df, output_dir_manager_list):\n",
    "\n",
    "\n",
    "    # Handle the duplicate rows for the same stocks, we sum up the shares and values, and save the file (13F, cik_company csv list)\n",
    "    if all_dfs:\n",
    "        final_df = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "        grouped_df = final_df.groupby([\"CIK\", \"CUSIP\",\"ACCESSION_NUMBER\"], as_index=False).agg({\n",
    "            \"FILING\": \"first\",\n",
    "            \"ACCESSION_NUMBER\": \"first\",\n",
    "            \"PERIODOFREPORT\": \"first\",\n",
    "            \"FILING_DATE\": \"first\",\n",
    "            \"FILINGMANAGER_NAME\": \"first\",\n",
    "            \"TITLEOFCLASS\": \"first\",\n",
    "            \"NAMEOFISSUER\": \"first\",\n",
    "            \"SSHPRNAMT\": \"sum\",\n",
    "            \"VALUE\": \"sum\"\n",
    "        })\n",
    "\n",
    "        grouped_df['PERCENT'] = grouped_df.groupby(\"CIK\")['VALUE'].transform(lambda x: x / x.sum())\n",
    "\n",
    "        grouped_df['CIK'] = grouped_df['CIK'].apply(lambda x: str(x).zfill(10))\n",
    "        current_date_time = datetime.now().strftime('%d-%b-%Y_%H-%M-%S').upper()\n",
    "        final_output_path = os.path.join(output_dir, f\"{current_date_time}.csv\")\n",
    "        columns = [\n",
    "            \"FILING\",\n",
    "            \"ACCESSION_NUMBER\",\n",
    "            \"PERIODOFREPORT\",\n",
    "            \"FILING_DATE\",\n",
    "            \"FILINGMANAGER_NAME\",\n",
    "            \"CIK\",\n",
    "            \"CUSIP\",\n",
    "            \"TITLEOFCLASS\",\n",
    "            \"NAMEOFISSUER\",\n",
    "            \"SSHPRNAMT\",\n",
    "            \"VALUE\",\n",
    "            \"PERCENT\"\n",
    "        ]\n",
    "        \n",
    "        \n",
    "\n",
    "        grouped_df = grouped_df[columns]\n",
    "        grouped_df.to_csv(final_output_path, index=False)\n",
    "\n",
    "        cik_company_output_path = os.path.join(output_dir_manager_list, f\"{current_date_time}_manager_list_date.csv\")\n",
    "        cik_company_df.to_csv(cik_company_output_path, index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new filings for CIK 200217\n",
      "No new filings for CIK 732905\n",
      "No new filings for CIK 783412\n",
      "No new filings for CIK 807249\n",
      "No new filings for CIK 807985\n",
      "No new filings for CIK 813917\n",
      "No new filings for CIK 814375\n",
      "No new filings for CIK 820124\n",
      "No new filings for CIK 846222\n",
      "No new filings for CIK 859804\n",
      "No new filings for CIK 860643\n",
      "No new filings for CIK 883965\n",
      "No new filings for CIK 898382\n",
      "No new filings for CIK 915191\n",
      "No new filings for CIK 921669\n",
      "No new filings for CIK 936753\n",
      "No new filings for CIK 945631\n",
      "No new filings for CIK 947996\n",
      "No new filings for CIK 948669\n",
      "No new filings for CIK 949509\n",
      "No new filings for CIK 1015079\n",
      "No new filings for CIK 1016287\n",
      "No new filings for CIK 1027796\n",
      "No new filings for CIK 1036325\n",
      "No new filings for CIK 1039565\n",
      "No new filings for CIK 1050442\n",
      "No new filings for CIK 1056831\n",
      "No new filings for CIK 1061165\n",
      "No new filings for CIK 1061768\n",
      "No new filings for CIK 1063296\n",
      "No new filings for CIK 1067983\n",
      "No new filings for CIK 1096343\n",
      "No new filings for CIK 1099281\n",
      "No new filings for CIK 1103804\n",
      "No new filings for CIK 1106129\n",
      "No new filings for CIK 1106500\n",
      "No new filings for CIK 1112520\n",
      "No new filings for CIK 1115373\n",
      "No new filings for CIK 1133219\n",
      "No new filings for CIK 1135778\n",
      "No new filings for CIK 1164833\n",
      "No new filings for CIK 1165797\n",
      "No new filings for CIK 1166559\n",
      "No new filings for CIK 1167483\n",
      "No new filings for CIK 1279936\n",
      "No new filings for CIK 1314620\n",
      "No new filings for CIK 1336528\n",
      "No new filings for CIK 1340807\n",
      "No new filings for CIK 1345471\n",
      "No new filings for CIK 1352662\n",
      "No new filings for CIK 1358706\n",
      "No new filings for CIK 1362535\n",
      "No new filings for CIK 1376879\n",
      "No new filings for CIK 1387366\n",
      "No new filings for CIK 1389403\n",
      "No new filings for CIK 1404599\n",
      "No new filings for CIK 1426749\n",
      "No new filings for CIK 1454502\n",
      "No new filings for CIK 1483866\n",
      "No new filings for CIK 1484148\n",
      "No new filings for CIK 1484150\n",
      "No new filings for CIK 1510387\n",
      "No new filings for CIK 1513193\n",
      "No new filings for CIK 1532262\n",
      "No new filings for CIK 1540866\n",
      "No new filings for CIK 1542280\n",
      "No new filings for CIK 1546190\n",
      "No new filings for CIK 1549575\n",
      "No new filings for CIK 1553733\n",
      "No new filings for CIK 1556785\n",
      "No new filings for CIK 1559771\n",
      "No new filings for CIK 1568621\n",
      "No new filings for CIK 1569205\n",
      "No new filings for CIK 1574850\n",
      "No new filings for CIK 1581811\n",
      "No new filings for CIK 1589943\n",
      "No new filings for CIK 1599814\n",
      "No new filings for CIK 1631014\n",
      "No new filings for CIK 1631664\n",
      "No new filings for CIK 1632958\n",
      "No new filings for CIK 1636974\n",
      "New fillings for CIK 1641864\n",
      "No new filings for CIK 1647251\n",
      "No new filings for CIK 1649339\n",
      "No new filings for CIK 1656456\n",
      "No new filings for CIK 1657335\n",
      "No new filings for CIK 1663801\n",
      "No new filings for CIK 1671657\n",
      "No new filings for CIK 1680843\n",
      "No new filings for CIK 1697591\n",
      "No new filings for CIK 1697868\n",
      "No new filings for CIK 1709323\n",
      "No new filings for CIK 1720350\n",
      "No new filings for CIK 1720792\n",
      "No new filings for CIK 1745214\n",
      "No new filings for CIK 1766504\n",
      "No new filings for CIK 1766596\n",
      "No new filings for CIK 1766908\n",
      "No new filings for CIK 1773994\n",
      "No new filings for CIK 1821168\n",
      "No new filings for CIK 1854794\n",
      "No new filings for CIK 1858353\n",
      "No new filings for CIK 1868537\n",
      "No new filings for CIK 1953324\n",
      "Filing: ['1641864']\n",
      "No Filing: ['200217', '732905', '783412', '807249', '807985', '813917', '814375', '820124', '846222', '859804', '860643', '883965', '898382', '915191', '921669', '936753', '945631', '947996', '948669', '949509', '1015079', '1016287', '1027796', '1036325', '1039565', '1050442', '1056831', '1061165', '1061768', '1063296', '1067983', '1096343', '1099281', '1103804', '1106129', '1106500', '1112520', '1115373', '1133219', '1135778', '1164833', '1165797', '1166559', '1167483', '1279936', '1314620', '1336528', '1340807', '1345471', '1352662', '1358706', '1362535', '1376879', '1387366', '1389403', '1404599', '1426749', '1454502', '1483866', '1484148', '1484150', '1510387', '1513193', '1532262', '1540866', '1542280', '1546190', '1549575', '1553733', '1556785', '1559771', '1568621', '1569205', '1574850', '1581811', '1589943', '1599814', '1631014', '1631664', '1632958', '1636974', '1647251', '1649339', '1656456', '1657335', '1663801', '1671657', '1680843', '1697591', '1697868', '1709323', '1720350', '1720792', '1745214', '1766504', '1766596', '1766908', '1773994', '1821168', '1854794', '1858353', '1868537', '1953324']\n",
      "There are 1 managers update new form\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "file_path = r\"C:\\Users\\Analyst\\Desktop\\Will\\13F\\13F Manager List\\21-AUG-2024_09-25-32_manager_list_date.csv\"\n",
    "cik_company_df = pd.read_csv(file_path)\n",
    "\n",
    "# Process CIKs and collect dataframes\n",
    "all_dfs, filing, no_filing = process_ciks(cik_company_df)\n",
    "\n",
    "# Save collected dataframes to CSV\n",
    "folder_name = \"13F Web Scrape\"\n",
    "output_dir = os.path.join(r\"C:\\Users\\Analyst\\Desktop\\Will\\13F\", folder_name)\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "output_dir_manager_list = r'C:\\Users\\Analyst\\Desktop\\Will\\13F\\13F Manager List'\n",
    "\n",
    "save_data_to_csv(all_dfs, output_dir, cik_company_df, output_dir_manager_list)\n",
    "\n",
    "print(\"Filing:\", filing)\n",
    "print(\"No Filing:\", no_filing)\n",
    "print('There are %s managers update new form' % len(filing) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
